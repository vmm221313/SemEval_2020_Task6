{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from make_graph import build_graphs\n",
    "from preprocess import load_and_preprocess\n",
    "\n",
    "df = load_and_preprocess()\n",
    "\n",
    "num_adj = 3\n",
    "\n",
    "graphs = build_graphs(df, num_adj)\n",
    "\n",
    "graphs[0].edges\n",
    "\n",
    "type(df['text'][i])\n",
    "\n",
    "# all_words = []\n",
    "# for i in tqdm_notebook(range(len(df))):\n",
    "#     all_words += df['text'][i]\n",
    "\n",
    "# len(all_words)\n",
    "\n",
    "# vocab = list(set(all_words))\n",
    "# len(vocab)\n",
    "\n",
    "# i = 0\n",
    "# adj_words = []\n",
    "# for j in range(len(df['text'][i])):\n",
    "#     for k in range(1, num_adj):\n",
    "#         adj_words.append((df['text'][i][j], df['text'][i][(j+k)%len(df['text'][i])]))\n",
    "#         adj_words.append((df['text'][i][j], df['text'][i][(j-k)%len(df['text'][i])]))\n",
    "\n",
    "# df['text'][0]\n",
    "\n",
    "# adj_words\n",
    "\n",
    "adj_words = []\n",
    "for i in tqdm_notebook(range(len(df))):\n",
    "    for j in range(len(df['text'][i])):\n",
    "        for k in range(1, num_adj):\n",
    "            adj_words.append((df['text'][i][j], df['text'][i][(j+k)%len(df['text'][i])]))\n",
    "            adj_words.append((df['text'][i][j], df['text'][i][(j-k)%len(df['text'][i])]))\n",
    "\n",
    "adj_words[:20]\n",
    "\n",
    "len(adj_words)\n",
    "\n",
    "adj_words = list(set(adj_words))\n",
    "\n",
    "len(adj_words)\n",
    "\n",
    "weights_dict = {}\n",
    "for i in tqdm_notebook(range(len(adj_words))):\n",
    "    weights_dict[adj_words[i]] = torch.randn(1)\n",
    "\n",
    "weights_dict\n",
    "\n",
    "len(weights_dict)\n",
    "\n",
    "with open('data/word_embeddings_BERT_cleaned_vocab', 'rb') as file:\n",
    "    word_embeddings = pickle.load(file)\n",
    "\n",
    "# G.add_edges_from([(df['text'][i][j], df['text'][i][(j+k)%len(df['text'][i])], {'weight': torch.randn(1)})])\n",
    "# G.add_edges_from([(df['text'][i][j], df['text'][i][(j-k)%len(df['text'][i])], {'weight': torch.randn(1)})])\n",
    "\n",
    "graphs_dict = {}\n",
    "for i in tqdm_notebook(range(len(df))):\n",
    "    G = nx.DiGraph()\n",
    "    for j in range(len(df['text'][i])):\n",
    "        G.add_node(df['text'][i][j], representation = word_embeddings[df['text'][i][j]])\n",
    "        \n",
    "    for j in range(len(df['text'][i])):\n",
    "        for k in range(1, num_adj):\n",
    "            word = df['text'][i][j]\n",
    "            next_word = df['text'][i][(j+k)%len(df['text'][i])]\n",
    "            prev_word = df['text'][i][(j-k)%len(df['text'][i])]\n",
    "            G.add_edges_from([(word, next_word, {'weight': weights_dict[word, next_word]})])\n",
    "            G.add_edges_from([(word, prev_word, {'weight': weights_dict[word, prev_word]})])\n",
    "    graphs_dict[i] = G\n",
    "\n",
    "graphs_dict[0].edges\n",
    "\n",
    "graphs_dict[0]['virtual']['two']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs[1].nodes\n",
    "\n",
    "graphs_copy = graphs\n",
    "\n",
    "for i in range(len(graphs_copy)):\n",
    "    print(graphs_copy[i].node['link'])\n",
    "    graphs_copy[i].node['link']['representation'] = 10*graphs_copy[i].node['link']['representation']\n",
    "    print(graphs_copy[i].node['link'])\n",
    "    break\n",
    "\n",
    "graphs_copy[i].node['link']['representation']\n",
    "\n",
    "for i in range(len(graphs_copy)):\n",
    "    if graphs_copy[i].has_node('link'):\n",
    "        graphs_copy[i].node['link']['representation'] = 10*graphs_copy[i].node['link']['representation']\n",
    "\n",
    "graphs_copy[2].node['link']\n",
    "\n",
    "\n",
    "\n",
    "graphs[0].edges\n",
    "\n",
    "##\n",
    "with open('data/weights_dict', 'rb') as weights_file:\n",
    "    weights_dict = pickle.load(weights_file)\n",
    "\n",
    "weights_dict\n",
    "\n",
    "graphs[1].number_of_nodes()\n",
    "\n",
    "graphs[1].number_of_edges()\n",
    "\n",
    "graphs[1].nodes\n",
    "\n",
    "graphs[1].node['graph']['representation']\n",
    "\n",
    "##\n",
    "edges = nx.to_dict_of_dicts(graphs[1])\n",
    "edges\n",
    "\n",
    "##\n",
    "with open('data/word_embeddings_BERT_cleaned_vocab', 'rb') as file:\n",
    "    word_embeddings = pickle.load(file)\n",
    "\n",
    "word_embeddings['graph'].shape\n",
    "\n",
    "len(word_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "node_param = {}\n",
    "for word in word_embeddings:\n",
    "    node_param[word] = torch.randn(1)\n",
    "\n",
    "#node_param and adj_nodes[node]['weight'] must be trainable\n",
    "\n",
    "##\n",
    "for edge in edges:\n",
    "    adj_nodes = edges[edge]\n",
    "    messages = torch.ones(len(adj_nodes), 1024)\n",
    "\n",
    "    for i, node in enumerate(adj_nodes):\n",
    "        messages[i, :] = adj_nodes[node]['weight'][0]*word_embeddings[node]\n",
    "\n",
    "    final_message = messages.max(dim = 0).values.view(-1, 1024)[0]\n",
    "    print(final_message - node_param[edge]*final_message + node_param[edge]*word_embeddings[edge])\n",
    "    word_embeddings[edge] = final_message - node_param[edge]*final_message + node_param[edge]*word_embeddings[edge]\n",
    "    \n",
    "    for i in range(len(graphs)):\n",
    "        if graphs[i].has_node(edge):\n",
    "            print('Updated')\n",
    "            graphs[i].node[edge]['representation'] = word_embeddings[edge]\n",
    "\n",
    "    break\n",
    "\n",
    "print(edge)\n",
    "\n",
    "word_embeddings[edge]\n",
    "\n",
    "word_embeddings[edge]\n",
    "\n",
    "adj_nodes\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "m = nn.Linear(1, 1)(final_message.T).T\n",
    "\n",
    "m.shape\n",
    "\n",
    "messages\n",
    "\n",
    "final_message\n",
    "\n",
    "final_message.shape\n",
    "\n",
    "messages.max(dim = 0).values\n",
    "\n",
    "word_embeddings['graph']\n",
    "\n",
    "a = torch.randn(3)\n",
    "b = torch.randn(3)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a.view(1,3).size()\n",
    "\n",
    "c = torch.randn(2,3)\n",
    "c[0, :] = a\n",
    "c[1, :] = b\n",
    "\n",
    "c\n",
    "\n",
    "a.numpy()\n",
    "\n",
    "b.numpy()\n",
    "\n",
    "b = torch.cat([a.view(1,3), b.view(1,3)], dim = 0)\n",
    "\n",
    "b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edges_df = nx.to_pandas_edgelist(graphs[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edges_df\n",
    "\n",
    "\n",
    "\n",
    "graph = graphs[1]\n",
    "\n",
    "edges = nx.to_dict_of_dicts(graph)\n",
    "\n",
    "nodes = graph.nodes\n",
    "sum_of_node_reps = 0\n",
    "for node in nodes:\n",
    "    print(node, graph.node[node]['representation'])\n",
    "    sum_of_node_reps += graph.node[node]['representation']\n",
    "\n",
    "sum_of_node_reps\n",
    "\n",
    "sum_of_node_reps.shape\n",
    "\n",
    "sum_of_node_reps.view(1, 1024).shape\n",
    "\n",
    "model(graph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['text'][1]\n",
    "\n",
    "\n",
    "    #message = \n",
    "    #print(edge, graphs[0][edge[0]][edge[1]])\n",
    "\n",
    "graphs[0]['link']['positive']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edges = nx.to_dict_of_dicts(graphs[i])\n",
    "\n",
    "            for edge in edges:\n",
    "                adj_nodes = edges[edge]\n",
    "\n",
    "            messages = torch.ones(len(adj_nodes), 1024)\n",
    "\n",
    "            for j, node in enumerate(adj_nodes):\n",
    "                messages[j, :] = adj_nodes[node]['weight'][0]*word_embeddings[node]\n",
    "\n",
    "            final_message = messages.max(dim = 0).values.view(-1, 1024)[0]\n",
    "            word_embeddings[edge] = final_message - node_param[edge]*final_message + node_param[edge]*word_embeddings[edge]\n",
    "\n",
    "            for j in range(len(graphs)):\n",
    "                if graphs[j].has_node(edge):\n",
    "                    graphs[j].node[edge]['representation'] = word_embeddings[edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(df))):\n",
    "    for j in range(len(df.iloc[i]['text'])):\n",
    "        df.iloc[i]['text'][j] = re.sub('\\.', '', df.iloc[i]['text'][j])\n",
    "\n",
    "df.to_csv('data/task6_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_embeddings_cleaned = {}\n",
    "for word in tqdm_notebook(word_embeddings):\n",
    "    cleaned_word = re.sub('values', '(values', word)\n",
    "    word_embeddings_cleaned[cleaned_word] = word_embeddings[word]\n",
    "\n",
    "with open('data/word_embeddings_BERT_cleaned_vocab', 'wb') as file:\n",
    "    pickle.dump(word_embeddings_cleaned, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/task6_data.csv', index = False)\n",
    "\n",
    "for graph in graphs:\n",
    "    if graphs[graph].number_of_nodes() == 0:\n",
    "        print('aerwg')\n",
    "\n",
    "len(graphs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_embeddings['nu(clear']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.iloc[4]['text']\n",
    "\n",
    "import re\n",
    "for i in tqdm_notebook(range(len(df))):\n",
    "    l = df.iloc[i]['text']\n",
    "    l_new = []\n",
    "    for j in range(len(l)):\n",
    "        l_new.append(re.sub('nuclear', 'nu(clear', l[j]))\n",
    "\n",
    "    \n",
    "    df.iloc[i]['text'] = l_new\n",
    "\n",
    "df.to_csv('data/task6_data.csv', index = False)\n",
    "\n",
    "import re\n",
    "word_embeddings_cleaned = {}\n",
    "for word in tqdm_notebook(word_embeddings):\n",
    "    cleaned_word = re.sub('keys', '(keys', word)\n",
    "    word_embeddings_cleaned[cleaned_word] = word_embeddings[word]\n",
    "\n",
    "with open('data/word_embeddings_BERT_cleaned_vocab', 'wb') as file:\n",
    "    pickle.dump(word_embeddings_cleaned, file)\n",
    "\n",
    "\n",
    "\n",
    "with open('data/word_embeddings_BERT_cleaned_vocab', 'wb') as file:\n",
    "    pickle.dump(word_embeddings_cleaned, file)\n",
    "\n",
    "a = nn.ParameterDict(node_param)\n",
    "\n",
    "node_param['apply']\n",
    "\n",
    "node_param\n",
    "\n",
    "dict_t = {'appl_y':torch.nn.Parameter(torch.tensor([3.4]))}\n",
    "\n",
    "node_param\n",
    "\n",
    "word_embeddings_cleaned\n",
    "\n",
    "df.iloc[0]['text']\n",
    "\n",
    "df.iloc[i]['text'][j]\n",
    "\n",
    "loss_function(torch.tensor([[2.6932, 3.6407]]), torch.tensor([0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "messages = torch.ones(len(adj_nodes), 1024)\n",
    "for j, node in enumerate(adj_nodes):\n",
    "    messages[j, :] = adj_nodes[node]['weight'][0]*word_embeddings[node]\n",
    "\n",
    "final_message = messages.max(dim = 0).values.view(-1, 1024)[0]\n",
    "word_embeddings[edge] = final_message - node_param[edge]*final_message + node_param[edge]*word_embeddings[edge]\n",
    "\n",
    "for j in range(len(graphs)):\n",
    "    if graphs[j].has_node(edge):\n",
    "        graphs[j].node[edge]['representation'] = word_embeddings[edge]\n",
    "\n",
    "adj_nodes\n",
    "\n",
    "edges = nx.to_dict_of_dicts(graphs[i])\n",
    "for edge in edges:\n",
    "    adj_nodes = edges[edge]\n",
    "\n",
    "i = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
